{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993ab2a6",
   "metadata": {},
   "source": [
    "# Introducción a PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761d885",
   "metadata": {},
   "source": [
    "### Cargando el entorno de PySpark en Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e702a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: spark-3.5.6-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.6-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39087e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'enciron'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1776413080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menciron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"JAVA_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/usr/lib/jvm/java-8-openjdk-amd64\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/spark-3.5.6-bin-hadoop3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'enciron'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.enciron[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.6-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cb43b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"ls\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b10269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .appName(\"Ejemplo\") \\\n",
    "                    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51738189",
   "metadata": {},
   "source": [
    "### Importando una base de datos externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e8682",
   "metadata": {},
   "source": [
    "### Creando un Dataframe a partir de un archivo \"csv\" de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239eb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"cars.csv\", header=True, sep=\";\", inferSchema=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimiendo el número de registros en el Dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbe77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimiendo el número de columnas en el Dataframe\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b23d5c",
   "metadata": {},
   "source": [
    "### Imprimiendo información sobre el tipo de datos y esquema del Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimiendo los tipos de datos del Dataframe\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimiendo el esquema del Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2350e",
   "metadata": {},
   "source": [
    "### Cambiando el esquema del Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se requiere cambiar el tipo de alguna columna, se puede usar withColumn\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# Dos formas en las cuales se puede cambiar el formato de la columna MPG\n",
    "df = df.withColumn('MPG', df['MPG'].cast(FloatType()))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d12720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si fuera necesario, se puede cambiar el tipo a todas las columnas del Dataframe usando selectExpr\n",
    "df2 = df.selectExpr(\n",
    "    'cast(Car as string) Car',\n",
    "    'cast(MPG as float) MPG',\n",
    "    'cast(Cylinders as int) Cylinders',\n",
    "    'cast(Displacement as int) Displacement',\n",
    "    'cast(Horsepower as int) Horsepower',\n",
    "    'cast(Weight as int) Weight',\n",
    "    'cast(Acceleration as float) Acceleration',\n",
    "    'cast(Model as int) Model',\n",
    "    'cast(Origin as string) Origin'\n",
    ")\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c97ac",
   "metadata": {},
   "source": [
    "### Definiendo un esquema para un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica un DDL\n",
    "ddl_schema = \"Car STRING, MPG FLOAT, Cylinders INT, Displacement FLOAT, Horsepower FLOAT, Weight FLOAT, Acceleration FLOAT, Model INT, Origin STRING\"\n",
    "\n",
    "df3 = spark.read.csv('cars.csv', header=True, sep=';', schema=ddl_schema)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b508f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se especificaun DDL que no coincide con el dataset, se pueden obtener lecturas nulas\n",
    "ddl_schema = \"Car STRING, MPG INT, Cylinders INT, Displacement FLOAT, Horsepower FLOAT, Weight FLOAT, Acceleration FLOAT, Model INT, Origin STRING\"\n",
    "\n",
    "df4 = spark.read.csv('cars.csv', header=True, sep=';', schema=ddl_schema)\n",
    "df4.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2b192",
   "metadata": {},
   "source": [
    "## Transformaciones aplicables a columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24404db0",
   "metadata": {},
   "source": [
    "### 1. Selección de columnas: select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4886c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la selección de columnas de un dataframe, se pueden usar variantes en la sintaxis\n",
    "print(\"Método 1\")\n",
    "df_car = df.select(df.Car)\n",
    "df_car.show(10, truncate=False)\n",
    "\n",
    "print(\"Método 2\")\n",
    "df_car = df.select(df['Car'])\n",
    "df_car.show(10, truncate=False)\n",
    "\n",
    "print(\"Método 3\")\n",
    "df_car = df.select('Car')\n",
    "df_car.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existe otra forma de acceder a una columna, usando el módulo \"col\" de SQL\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Método 4\")\n",
    "df.select(col('car')).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es posible seleccionar varias columnas en una sola transformación\n",
    "# Nota: se puede mezclar sintaxis\n",
    "\n",
    "# Selección de columa \"Car\" y \"MPG\"\n",
    "# Método 1\n",
    "print(\"Método 1\")\n",
    "df_Car_MPG = df.select(df.Car, df.MPG)\n",
    "df_Car_MPG.show(10, truncate=False)\n",
    "\n",
    "# Método 2\n",
    "print(\"Método 2\")\n",
    "df_Car_MPG = df.select(df['Car'], df.MPG)\n",
    "df_Car_MPG.show(10, truncate=False)\n",
    "\n",
    "# Método 3\n",
    "print(\"Método 3\")\n",
    "from pyspark.sql.functions import col\n",
    "df_Car_MPG = df.select(col('car'), col('mpg'))\n",
    "df_Car_MPG.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14cd6c",
   "metadata": {},
   "source": [
    "### 2. Añadiendo columnas: withColumn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso 2: Añadiendo la columna \"col_1\"\n",
    "from pyspark.sql.functions import lit\n",
    "df_newCols = df.withColumn('col_1', lit(1))\n",
    "# lit se usa para especificar el valor a usar para llenar la columna creada\n",
    "\n",
    "# Caso 2: Añadiendo dos columnas\n",
    "df_newCols = df_newCols.withColumn('col_2', lit(2))  \\\n",
    "                       .withColumn('col_3', lit(3))\n",
    "df_newCols.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso 3: Añadiendo una nueva columna a partir de una existente\n",
    "# La nueva columna se llamará 'car_model' generada a partir de las columnas Car y Model\n",
    "from pyspark.sql.functions import concat\n",
    "df_newCols = df_newCols.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"Model\")))\n",
    "\n",
    "df_newCols.show(10, truncate=False)\n",
    "print(\"# registros:\", df_newCols.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
